{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2879b9eb",
   "metadata": {},
   "source": [
    "- **IMPORTING MODULES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79a68a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018920a",
   "metadata": {},
   "source": [
    "- **TASKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b3610",
   "metadata": {},
   "source": [
    "- **Part A: Conceptual Foundation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244fb75",
   "metadata": {},
   "source": [
    "1. Write short notes on:\n",
    "\n",
    "• What is Data Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7df9e1",
   "metadata": {},
   "source": [
    "- Data Analysis means collecting, cleaning, and studying data to find useful information. It helps us understand patterns, trends, and relationships in data so that better decisions can be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b5095",
   "metadata": {},
   "source": [
    "• How to Plan a Data Science Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c1d2a",
   "metadata": {},
   "source": [
    "- A typical Data Science Project Plan looks like this:\n",
    "\n",
    "- Define the Problem – Understand what you’re trying to solve.\n",
    "\n",
    "- Collect Data – Gather all relevant data.\n",
    "\n",
    "- Explore and Clean Data – Handle missing values, outliers, and inconsistent formats.\n",
    "\n",
    "- Feature Engineering – Create or modify features that improve model performance.\n",
    "\n",
    "- Model Building – Choose and train machine learning models.\n",
    "\n",
    "- Model Evaluation – Check how well the model performs.\n",
    "\n",
    "- Deployment – Use the model in a real system or application.\n",
    "\n",
    "- Monitoring & Maintenance – Track performance and update the model when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c95d3",
   "metadata": {},
   "source": [
    "• How to Frame a Machine Learning Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f823c85",
   "metadata": {},
   "source": [
    "- Steps to Frame a ML Problem:\n",
    "\n",
    "- Understand the Goal – What do we want to predict or classify?\n",
    "\n",
    "- Identify Inputs (Features) – What information do we have to make predictions?\n",
    "\n",
    "- Identify Output (Target) – What outcome do we want to predict?\n",
    "\n",
    "- Select ML Type:\n",
    "    - upervised Learning – When we have both input and output data (e.g., predict loan default).\n",
    "    - Unsupervised Learning – When we only have inputs (e.g., group customers by spending habits).\n",
    "\n",
    "- Define Evaluation Metrics – How will we measure success? (accuracy, RMSE, F1 score, etc.)\n",
    "\n",
    "- Consider Constraints – Data availability, time, computing power, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176ff2c",
   "metadata": {},
   "source": [
    "2. Explain Tensors and provide an in-depth explanation with NumPy examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8af282",
   "metadata": {},
   "source": [
    "- A Tensor is a container for numerical data — like scalars, vectors, and matrices — that can have more than two dimensions.\n",
    "\n",
    "- They are used a lot in machine learning and deep learning (like in TensorFlow or PyTorch).\n",
    "\n",
    "- Types of Tensors:\n",
    "    - Scalar (0D Tensor): A single number.\n",
    "    - Vector (1D Tensor): A list of numbers.\n",
    "    - Matrix (2D Tensor): A table of numbers (rows and columns).\n",
    "    - 3D or Higher Tensors: Collections of matrices (used in images, videos, etc.).\n",
    "\n",
    "- Example:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tensor_3d = np.array([\n",
    "\n",
    "[[1, 2], [3, 4]],\n",
    "\n",
    "[[5, 6], [7, 8]]\n",
    "\n",
    "])\n",
    "\n",
    "print(\"3D Tensor:\\n\", tensor_3d)\n",
    "\n",
    "3D Tensor:\n",
    "\n",
    "[[[1 2]\n",
    "\n",
    "[3 4]]\n",
    "\n",
    "[[5 6]\n",
    "\n",
    "[7 8]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa615172",
   "metadata": {},
   "source": [
    "- **Part B: Data Acquisition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f17d5",
   "metadata": {},
   "source": [
    "3. Import datasets from multiple sources:\n",
    "\n",
    "- Load CSV files (main transactions dataset).\n",
    "\n",
    "- Parse JSON files (customer metadata).\n",
    "\n",
    "- Fetch records from SQL (loan repayment history).\n",
    "\n",
    "- Fetch data from a dummy API (external economic indicators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3bcab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "data_csv = pd.read_csv(\"credit_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8aab23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "data_json = pd.read_json(\"customer_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26ad4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from a dummy API(external economic indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19196af",
   "metadata": {},
   "source": [
    "- **Part C: Data Understanding & Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac41bc8",
   "metadata": {},
   "source": [
    "\n",
    "4. Explore the dataset using Pandas (info(), describe())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a400922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   customer_id        1000 non-null   int64  \n",
      " 1   age                920 non-null    float64\n",
      " 2   gender             950 non-null    object \n",
      " 3   region             1000 non-null   object \n",
      " 4   education_level    1000 non-null   object \n",
      " 5   employment_type    930 non-null    object \n",
      " 6   annual_income      940 non-null    float64\n",
      " 7   loan_amount        960 non-null    float64\n",
      " 8   loan_purpose       1000 non-null   object \n",
      " 9   credit_score       950 non-null    float64\n",
      " 10  repayment_history  1000 non-null   int64  \n",
      " 11  transaction_count  1000 non-null   int64  \n",
      " 12  spending_ratio     1000 non-null   float64\n",
      " 13  join_date          1000 non-null   object \n",
      " 14  default_flag       1000 non-null   int64  \n",
      "dtypes: float64(5), int64(4), object(6)\n",
      "memory usage: 117.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_csv.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc75f10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         customer_id         age  annual_income   loan_amount  credit_score  \\\n",
      "count    1000.000000  920.000000     940.000000  9.600000e+02    950.000000   \n",
      "mean   100499.500000   35.290217   62178.613044  2.633497e+04    647.743158   \n",
      "std       288.819436    9.496320   66307.077136  6.885842e+04     70.360530   \n",
      "min    100000.000000   18.000000    4373.610000  5.133500e+02    250.000000   \n",
      "25%    100249.750000   28.000000   24076.677500  7.484122e+03    607.000000   \n",
      "50%    100499.500000   35.000000   42389.430000  1.469949e+04    647.000000   \n",
      "75%    100749.250000   41.000000   77030.370000  2.805263e+04    690.000000   \n",
      "max    100999.000000   74.000000  728180.593989  1.808443e+06    950.000000   \n",
      "\n",
      "       repayment_history  transaction_count  spending_ratio  default_flag  \n",
      "count          1000.0000        1000.000000     1000.000000    1000.00000  \n",
      "mean              0.5990          60.094000       31.823288       0.12100  \n",
      "std               1.0016          51.167874       20.210652       0.32629  \n",
      "min               0.0000          23.000000        0.334974       0.00000  \n",
      "25%               0.0000          36.000000       18.135623       0.00000  \n",
      "50%               0.0000          41.000000       29.896367       0.00000  \n",
      "75%               1.0000          48.000000       42.507198       0.00000  \n",
      "max              12.0000         244.000000      252.939139       1.00000  \n"
     ]
    }
   ],
   "source": [
    "print(data_csv.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1297d4",
   "metadata": {},
   "source": [
    "5. Perform Pandas Profiling to generate a data quality report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67f48fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing pandas profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7b2bb",
   "metadata": {},
   "source": [
    "6. Handle missing data with:\n",
    "    - Simple Imputer (numerical: mean/median).\n",
    "    - Simple Imputer (categorical: most frequent).\n",
    "    - Most Frequent Category Imputation.\n",
    "    - Missing Indicator + Random Sample Imputation.\n",
    "    - KNN Imputer (multivariate).\n",
    "    - MICE Algorithm.\n",
    "    - Complete Case Analysis (dropping rows/columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bf38830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer for numerical data\n",
    "from sklearn.impute import SimpleImputer\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "data_csv[['age', 'income']] = num_imputer.fit_transform(data_csv[['age', 'annual_income']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdd1dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer for categorical data\n",
    "from sklearn.impute import SimpleImputer\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_csv[['gender']] = cat_imputer.fit_transform(data_csv[['gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "436c06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent category imputation (impute categorical columns by column names)\n",
    "# 'employment_type' has missing values in this dataset — impute it using the imputer\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "data_csv[['employment_type']] = cat_imputer.fit_transform(data_csv[['employment_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "798c7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing indicator + Random sample imputation\n",
    "from sklearn.impute import MissingIndicator\n",
    "missing_indicator = MissingIndicator()\n",
    "data_csv[['credit_score']] = missing_indicator.fit_transform(data_csv[['credit_score']])\n",
    "random_sample = data_csv['credit_score'].dropna().sample(data_csv['credit_score'].isnull().sum(), random_state=0)\n",
    "data_csv.loc[data_csv['credit_score'].isnull(), 'credit_score'] = random_sample.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ceb8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Imputer\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c48cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICE Algorithm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12d4a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete case analysis (dropping(rows/columns)\n",
    "# Dropping rows with missing values\n",
    "data_csv_dropped_rows = data_csv.dropna()\n",
    "\n",
    "# Dropping columns with missing values\n",
    "data_csv_dropped_columns = data_csv.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d2d49",
   "metadata": {},
   "source": [
    "- **Part D: Outlier Handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4186c",
   "metadata": {},
   "source": [
    "- 7. Detect and treat outliers using:\n",
    "    - Z-score Method.\n",
    "    - IQR Method.\n",
    "    - Percentile Method.\n",
    "    - Winsorization Technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9294230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers using Z-score method\n",
    "from scipy import stats\n",
    "threshold = 3\n",
    "z_scores = np.abs(stats.zscore(data_csv.select_dtypes(include=[np.number])))\n",
    "outliers = np.where(z_scores > threshold)\n",
    "data_csv_no_outliers = data_csv[(z_scores < threshold).all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "634522d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers using IQR method\n",
    "\n",
    "for cols in data_csv.select_dtypes(include=[np.number]).columns:\n",
    "    Q1 = data_csv[cols].quantile(0.25)\n",
    "    Q3 = data_csv[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    data_csv = data_csv[(data_csv[cols] >= lower_limit) & (data_csv[cols] <= upper_limit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04699ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers using percentile method\n",
    "lower_percentile = 0.01\n",
    "upper_percentile = 0.99\n",
    "for cols in data_csv.select_dtypes(include=[np.number]).columns:\n",
    "    lower_limit = data_csv[cols].quantile(lower_percentile)\n",
    "    upper_limit = data_csv[cols].quantile(upper_percentile)\n",
    "    data_csv = data_csv[(data_csv[cols] >= lower_limit) & (data_csv[cols] <= upper_limit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "254d1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers using winsorization method\n",
    "from scipy.stats.mstats import winsorize\n",
    "for cols in data_csv.select_dtypes(include=[np.number]).columns:\n",
    "    data_csv[cols] = winsorize(data_csv[cols], limits=[0.01, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61215890",
   "metadata": {},
   "source": [
    "- **Part E: Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041ea01",
   "metadata": {},
   "source": [
    "- **8. Handle variable types:**\n",
    "    - Mixed Variables (numeric + categorical).\n",
    "    - Date & Time variables → extract Year, Month, Day, Weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "64dccddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle variable types:\n",
    "# Date & Time extract year, month, day, weekday\n",
    "data_csv['account_creation_date'] = pd.to_datetime(data_csv['join_date'])\n",
    "data_csv['account_creation_year'] = data_csv['account_creation_date'].dt.year\n",
    "data_csv['account_creation_month'] = data_csv['account_creation_date'].dt.month\n",
    "data_csv['account_creation_day'] = data_csv['account_creation_date'].dt.day\n",
    "data_csv['account_creation_weekday'] = data_csv['account_creation_date'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538226bd",
   "metadata": {},
   "source": [
    "- **9. Encoding categorical variables:**\n",
    "    - Ordinal Encoding (education levels).\n",
    "    - Label Encoding (binary features).\n",
    "    - One-Hot Encoding (regions, loan purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "767aa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "# Ordinal Encoding(education levels)\n",
    "education_mapping = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\n",
    "data_csv['education_level_encoded'] = data_csv['education_level'].map(education_mapping)\n",
    "\n",
    "# label encoding(binary features)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data_csv['gender_encoded'] = label_encoder.fit_transform(data_csv['gender'])\n",
    "\n",
    "# One-Hot encoding(regions, loan purpose)\n",
    "data_csv = pd.get_dummies(data_csv, columns=['region', 'loan_purpose'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad39c3",
   "metadata": {},
   "source": [
    "- **10. Encoding numerical features:**\n",
    "    - Binning (discretize income into groups).\n",
    "    - Binarization (flag if > threshold).\n",
    "    - Quantile Binning.\n",
    "    - K-Means Binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70b18147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding numerical features \n",
    "# Binning (discretization income into groups)\n",
    "data_csv['income_bin'] = pd.cut(data_csv['annual_income'], bins=[0, 30000, 60000, 90000, 120000, np.inf], labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Binarization(flag if > threshold)\n",
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=50000)\n",
    "data_csv['high_income_flag'] = binarizer.fit_transform(data_csv[['annual_income']])\n",
    "\n",
    "# Qunatile binning\n",
    "data_csv['income_quantile'] = pd.qcut(data_csv['annual_income'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# K-means binning\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "data_csv['income_kmeans_bin'] = kmeans.fit_predict(data_csv[['annual_income']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab767d",
   "metadata": {},
   "source": [
    "- **Part F: Feature Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5068cf7",
   "metadata": {},
   "source": [
    "- **11. Apply multiple scaling methods:**\n",
    "    - Standardization (Z-score scaling).\n",
    "    - Normalization.\n",
    "    - Min-Max Scaling.\n",
    "    - MaxAbs Scaling.\n",
    "    - Robust Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22cb3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying multiple scaling methods\n",
    "# Standardization(Z-score scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_csv[['age', 'annual_income', 'credit_score']] = scaler.fit_transform(data_csv[['age', 'annual_income', 'credit_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7873364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "data_csv[['age', 'annual_income', 'credit_score']] = scaler.fit_transform(data_csv[['age', 'annual_income', 'credit_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80ad167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_csv[['age', 'annual_income', 'credit_score']] = scaler.fit_transform(data_csv[['age', 'annual_income', 'credit_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1d6ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Abs Scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "data_csv[['age', 'annual_income', 'credit_score']] = scaler.fit_transform(data_csv[['age', 'annual_income', 'credit_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fcbf0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "data_csv[['age', 'annual_income', 'credit_score']] = scaler.fit_transform(data_csv[['age', 'annual_income', 'credit_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ca773",
   "metadata": {},
   "source": [
    "- **Part G: Feature Construction & Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9126b",
   "metadata": {},
   "source": [
    "- **12. Apply transformations:**\n",
    "    - FunctionTransformer → log transform, reciprocal, square root.\n",
    "    - PowerTransformer → Box-Cox and Yeo-Johnson.\n",
    "    - Column Transformer → apply different preprocessing steps to different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29d0c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Functional transformer and Power transformer\n",
    "# Defining the transformers\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False)\n",
    "reci_transformer = FunctionTransformer(lambda x: 1 / (x + 1), validate=False)\n",
    "sqrt_transformer = FunctionTransformer(np.sqrt, validate=False)\n",
    "boxcox_transformer = PowerTransformer(method='box-cox', standardize=False)\n",
    "yeo_johnson_transformer = PowerTransformer(method='yeo-johnson', standardize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953ede3",
   "metadata": {},
   "source": [
    "- **13. Construct new features:**\n",
    "    - Debt-to-Income ratio.\n",
    "    - Average monthly transactions.\n",
    "    - Spending-to-Income ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9cd88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct new features\n",
    "# Debt to Income Ratio\n",
    "data_csv['debt_to_income_ratio'] = data_csv['loan_amount'] / data_csv['annual_income']\n",
    "\n",
    "# Average monthly transactions\n",
    "data_csv['avg_monthly_transactions'] = data_csv['transaction_count'] / data_csv['account_creation_month']\n",
    "\n",
    "# Spending to Income Ratio\n",
    "data_csv['spending_to_income_ratio'] = data_csv['spending_ratio'] / data_csv['annual_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5854a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the transformers using ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log', log_transformer, ['annual_income']),\n",
    "        ('reci', reci_transformer, ['credit_score']),\n",
    "        ('sqrt', sqrt_transformer, ['age']),\n",
    "        ('boxcox', boxcox_transformer, ['loan_amount']),\n",
    "        ('yeo_johnson', yeo_johnson_transformer, ['spending_ratio'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa5cfa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:395: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Applying transformations\n",
    "tranformed_data_csv = column_transformer.fit_transform(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "17645800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new transformed DataFrame\n",
    "final_df = pd.DataFrame(tranformed_data_csv, columns=data_csv.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b97be7",
   "metadata": {},
   "source": [
    "- **Part H: Final Deliverable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b4642",
   "metadata": {},
   "source": [
    "- **14. Provide a final cleaned and transformed dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a03f4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned dataset saved as 'cleaned_transformed_credit_dataset.csv'\n",
      "Final Shape:  (480, 35)\n"
     ]
    }
   ],
   "source": [
    "# Providing a final cleaned and transformed dataset\n",
    "final_df.to_csv(\"cleaned_transformed_credit_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Final cleaned dataset saved as 'cleaned_transformed_credit_dataset.csv'\")\n",
    "print(\"Final Shape: \", final_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2780a8b",
   "metadata": {},
   "source": [
    "- **15. Write a report summarizing:**\n",
    "    - Missing value strategies used and their effectiveness.\n",
    "    - Outlier handling results.\n",
    "    - Encoding methods applied to categorical/numerical variables.\n",
    "    - Scaling transformations applied and why.\n",
    "    - Newly engineered features and their usefulness.\n",
    "    - Final dataset shape and readiness for ML modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1397d2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report is ready and it is saved as in WORD file!\n",
      "Data processing and feature engineering completed successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"The report is ready and it is saved as in WORD file!\")\n",
    "print(\"Data processing and feature engineering completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
